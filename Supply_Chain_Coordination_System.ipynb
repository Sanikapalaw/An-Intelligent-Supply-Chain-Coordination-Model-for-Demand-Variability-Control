{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¯ INTELLIGENT SUPPLY CHAIN COORDINATION SYSTEM\n",
    "## Mitigating Bullwhip Effect using ML/DL/RL\n",
    "\n",
    "---\n",
    "\n",
    "**Problem Statement:**  \n",
    "Lack of effective coordination among supply chain partners leads to distorted demand information, increased variability, late deliveries, excess inventory, and inefficient operations across the supply chain. Traditional supply chain systems operate in isolated stages where forecasting, ordering, and replenishment decisions are taken independently, resulting in the bullwhip effect and poor synchronization between supply and demand.\n",
    "\n",
    "**Solution:**  \n",
    "An intelligent coordination system that can accurately forecast demand, predict delivery risks, and support adaptive decision-making to reduce variability and improve overall supply chain performance.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Table of Contents:\n",
    "1. Setup & Data Loading\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Feature Engineering\n",
    "4. Bullwhip Effect Analysis\n",
    "5. Demand Forecasting (LSTM)\n",
    "6. Delivery Risk Prediction (XGBoost)\n",
    "7. Geographic Analysis & Clustering\n",
    "8. Reinforcement Learning (DQN)\n",
    "9. Coordination Recommendations\n",
    "10. Model Saving & Results\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 1: SETUP & DATA LOADING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed)\n",
    "# !pip install pandas numpy matplotlib seaborn plotly folium xgboost scikit-learn tensorflow imbalanced-learn networkx prophet tqdm\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1.1 IMPORT LIBRARIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve,\n",
    "    mean_squared_error, mean_absolute_error, r2_score\n",
    ")\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "\n",
    "# Model Saving\n",
    "import pickle\n",
    "import joblib\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"âœ… ALL LIBRARIES IMPORTED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)\n",
    "print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1.2 LOAD DATACO DATASET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# IMPORTANT: Upload your DataCo CSV file to Colab first!\n",
    "# Click on the folder icon on the left -> Upload\n",
    "\n",
    "# Option 1: If file is in Colab root directory\n",
    "FILE_PATH = 'DataCoSupplyChainDataset.csv'  # Change this to your filename\n",
    "\n",
    "# Option 2: If using Google Drive (uncomment below)\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# FILE_PATH = '/content/drive/MyDrive/DataCoSupplyChainDataset.csv'\n",
    "\n",
    "print(\"Loading DataCo Supply Chain Dataset...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(FILE_PATH, encoding='latin-1')  # Try latin-1 if utf-8 fails\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"âœ… DATASET LOADED SUCCESSFULLY!\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nğŸ“Š Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "    print(f\"\\nğŸ“‹ Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ ERROR: File not found!\")\n",
    "    print(\"\\nPlease upload the DataCo dataset CSV file to Colab.\")\n",
    "    print(\"Instructions:\")\n",
    "    print(\"1. Click the folder icon on the left sidebar\")\n",
    "    print(\"2. Click the upload button\")\n",
    "    print(\"3. Select your DataCoSupplyChainDataset.csv file\")\n",
    "    print(\"4. Update FILE_PATH variable above if needed\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 1.3 INITIAL DATA OVERVIEW\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ COLUMN NAMES\")\n",
    "print(\"=\"*60)\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i:3d}. {col}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ” FIRST 5 ROWS\")\n",
    "print(\"=\"*60)\n",
    "display(df.head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“Š DATA TYPES\")\n",
    "print(\"=\"*60)\n",
    "print(df.dtypes)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“ˆ BASIC STATISTICS\")\n",
    "print(\"=\"*60)\n",
    "display(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 2: EXPLORATORY DATA ANALYSIS (EDA)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.1 DATA CLEANING & PREPROCESSING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Starting data cleaning...\\n\")\n",
    "\n",
    "# Store original shape\n",
    "original_shape = df.shape\n",
    "\n",
    "# 1. Check for missing values\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ” MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing_Count': missing_values,\n",
    "    'Percentage': missing_percent\n",
    "})\n",
    "missing_df = missing_df[missing_df['Missing_Count'] > 0].sort_values('Missing_Count', ascending=False)\n",
    "\n",
    "if len(missing_df) > 0:\n",
    "    print(missing_df)\n",
    "    \n",
    "    # Drop columns with >50% missing values\n",
    "    cols_to_drop = missing_df[missing_df['Percentage'] > 50].index.tolist()\n",
    "    if cols_to_drop:\n",
    "        print(f\"\\nâš ï¸  Dropping columns with >50% missing: {cols_to_drop}\")\n",
    "        df = df.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Fill remaining missing values\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().sum() > 0:\n",
    "            if df[col].dtype in ['float64', 'int64']:\n",
    "                df[col].fillna(df[col].median(), inplace=True)\n",
    "            else:\n",
    "                df[col].fillna(df[col].mode()[0], inplace=True)\n",
    "else:\n",
    "    print(\"âœ… No missing values found!\")\n",
    "\n",
    "# 2. Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\nğŸ” Duplicate Rows: {duplicates}\")\n",
    "if duplicates > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"âœ… Removed {duplicates} duplicate rows\")\n",
    "\n",
    "# 3. Standardize column names (remove spaces, lowercase)\n",
    "df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "print(f\"\\nğŸ“Š Shape after cleaning: {df.shape}\")\n",
    "print(f\"ğŸ“Š Original shape: {original_shape}\")\n",
    "print(f\"âœ… Rows removed: {original_shape[0] - df.shape[0]}\")\n",
    "\n",
    "# Display cleaned data info\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ğŸ“‹ CLEANED DATA INFO\")\n",
    "print(\"=\"*60)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.2 KEY METRICS OVERVIEW\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Identify key columns (adjust based on actual column names)\n",
    "# Common DataCo columns:\n",
    "key_cols = {\n",
    "    'order_item_quantity': 'Order Quantity',\n",
    "    'sales': 'Sales',\n",
    "    'late_delivery_risk': 'Late Delivery Risk',\n",
    "    'days_for_shipping_real': 'Actual Shipping Days',\n",
    "    'days_for_shipment_scheduled': 'Scheduled Shipping Days',\n",
    "    'order_status': 'Order Status',\n",
    "    'shipping_mode': 'Shipping Mode',\n",
    "    'market': 'Market',\n",
    "    'order_region': 'Region',\n",
    "    'customer_segment': 'Customer Segment',\n",
    "    'category_name': 'Category',\n",
    "}\n",
    "\n",
    "# Check which columns exist in the dataset\n",
    "existing_cols = {k: v for k, v in key_cols.items() if k in df.columns}\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ¯ KEY SUPPLY CHAIN METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if 'order_item_quantity' in df.columns:\n",
    "    print(f\"\\nğŸ“¦ Total Orders: {len(df):,}\")\n",
    "    print(f\"ğŸ“¦ Total Quantity Ordered: {df['order_item_quantity'].sum():,.0f}\")\n",
    "    print(f\"ğŸ“¦ Avg Order Quantity: {df['order_item_quantity'].mean():.2f}\")\n",
    "\n",
    "if 'sales' in df.columns:\n",
    "    print(f\"\\nğŸ’° Total Sales: ${df['sales'].sum():,.2f}\")\n",
    "    print(f\"ğŸ’° Avg Sales per Order: ${df['sales'].mean():.2f}\")\n",
    "\n",
    "if 'late_delivery_risk' in df.columns:\n",
    "    late_risk = df['late_delivery_risk'].value_counts()\n",
    "    print(f\"\\nğŸšš Late Delivery Risk:\")\n",
    "    print(f\"   - High Risk: {late_risk.get(1, 0):,} ({late_risk.get(1, 0)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   - Low Risk: {late_risk.get(0, 0):,} ({late_risk.get(0, 0)/len(df)*100:.1f}%)\")\n",
    "\n",
    "if 'days_for_shipping_real' in df.columns and 'days_for_shipment_scheduled' in df.columns:\n",
    "    df['shipping_delay'] = df['days_for_shipping_real'] - df['days_for_shipment_scheduled']\n",
    "    print(f\"\\nâ±ï¸  Shipping Performance:\")\n",
    "    print(f\"   - Avg Actual Days: {df['days_for_shipping_real'].mean():.2f}\")\n",
    "    print(f\"   - Avg Scheduled Days: {df['days_for_shipment_scheduled'].mean():.2f}\")\n",
    "    print(f\"   - Avg Delay: {df['shipping_delay'].mean():.2f} days\")\n",
    "\n",
    "if 'market' in df.columns:\n",
    "    print(f\"\\nğŸŒ Markets Served: {df['market'].nunique()}\")\n",
    "    print(df['market'].value_counts())\n",
    "\n",
    "if 'order_region' in df.columns:\n",
    "    print(f\"\\nğŸ“ Regions: {df['order_region'].nunique()}\")\n",
    "\n",
    "if 'shipping_mode' in df.columns:\n",
    "    print(f\"\\nğŸš› Shipping Modes:\")\n",
    "    print(df['shipping_mode'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.3 VISUALIZATIONS - DISTRIBUTION ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Create subplots for key distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "fig.suptitle('Supply Chain Key Metrics Distribution', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Order Quantity Distribution\n",
    "if 'order_item_quantity' in df.columns:\n",
    "    axes[0, 0].hist(df['order_item_quantity'], bins=50, color='skyblue', edgecolor='black')\n",
    "    axes[0, 0].set_title('Order Quantity Distribution')\n",
    "    axes[0, 0].set_xlabel('Quantity')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "# 2. Sales Distribution\n",
    "if 'sales' in df.columns:\n",
    "    axes[0, 1].hist(df['sales'], bins=50, color='lightgreen', edgecolor='black')\n",
    "    axes[0, 1].set_title('Sales Distribution')\n",
    "    axes[0, 1].set_xlabel('Sales ($)')\n",
    "    axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "# 3. Late Delivery Risk\n",
    "if 'late_delivery_risk' in df.columns:\n",
    "    risk_counts = df['late_delivery_risk'].value_counts()\n",
    "    axes[0, 2].bar(['Low Risk', 'High Risk'], risk_counts.values, color=['green', 'red'])\n",
    "    axes[0, 2].set_title('Late Delivery Risk')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "\n",
    "# 4. Shipping Days - Real vs Scheduled\n",
    "if 'days_for_shipping_real' in df.columns and 'days_for_shipment_scheduled' in df.columns:\n",
    "    axes[1, 0].hist(df['days_for_shipping_real'], bins=30, alpha=0.7, label='Actual', color='orange')\n",
    "    axes[1, 0].hist(df['days_for_shipment_scheduled'], bins=30, alpha=0.7, label='Scheduled', color='blue')\n",
    "    axes[1, 0].set_title('Shipping Days: Actual vs Scheduled')\n",
    "    axes[1, 0].set_xlabel('Days')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].legend()\n",
    "\n",
    "# 5. Shipping Delay Distribution\n",
    "if 'shipping_delay' in df.columns:\n",
    "    axes[1, 1].hist(df['shipping_delay'], bins=50, color='coral', edgecolor='black')\n",
    "    axes[1, 1].set_title('Shipping Delay Distribution')\n",
    "    axes[1, 1].set_xlabel('Delay (days)')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].axvline(x=0, color='red', linestyle='--', label='On Time')\n",
    "    axes[1, 1].legend()\n",
    "\n",
    "# 6. Market Distribution\n",
    "if 'market' in df.columns:\n",
    "    market_counts = df['market'].value_counts().head(10)\n",
    "    axes[1, 2].barh(market_counts.index, market_counts.values, color='purple')\n",
    "    axes[1, 2].set_title('Top Markets')\n",
    "    axes[1, 2].set_xlabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"âœ… Distribution visualizations complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 2.4 CORRELATION ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Select numerical columns for correlation\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove ID columns\n",
    "numerical_cols = [col for col in numerical_cols if 'id' not in col.lower()]\n",
    "\n",
    "if len(numerical_cols) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1, cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find strong correlations (>0.7 or <-0.7)\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ğŸ” STRONG CORRELATIONS (|r| > 0.7)\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    strong_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            if abs(correlation_matrix.iloc[i, j]) > 0.7:\n",
    "                strong_corr.append((\n",
    "                    correlation_matrix.columns[i],\n",
    "                    correlation_matrix.columns[j],\n",
    "                    correlation_matrix.iloc[i, j]\n",
    "                ))\n",
    "    \n",
    "    if strong_corr:\n",
    "        for var1, var2, corr in sorted(strong_corr, key=lambda x: abs(x[2]), reverse=True):\n",
    "            print(f\"{var1} <-> {var2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"No strong correlations found.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Not enough numerical columns for correlation analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 3: FEATURE ENGINEERING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.1 DATE/TIME FEATURES EXTRACTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Creating date/time features...\\n\")\n",
    "\n",
    "# Find date columns\n",
    "date_columns = [col for col in df.columns if 'date' in col.lower() or 'time' in col.lower()]\n",
    "print(f\"Date columns found: {date_columns}\")\n",
    "\n",
    "# Process order date if exists\n",
    "if 'order_date_dateorders' in df.columns or 'order_date' in df.columns:\n",
    "    date_col = 'order_date_dateorders' if 'order_date_dateorders' in df.columns else 'order_date'\n",
    "    \n",
    "    # Convert to datetime\n",
    "    df[date_col] = pd.to_datetime(df[date_col], errors='coerce')\n",
    "    \n",
    "    # Extract features\n",
    "    df['order_year'] = df[date_col].dt.year\n",
    "    df['order_month'] = df[date_col].dt.month\n",
    "    df['order_day'] = df[date_col].dt.day\n",
    "    df['order_dayofweek'] = df[date_col].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['order_quarter'] = df[date_col].dt.quarter\n",
    "    df['order_week'] = df[date_col].dt.isocalendar().week\n",
    "    df['is_weekend'] = df['order_dayofweek'].isin([5, 6]).astype(int)\n",
    "    df['is_month_start'] = df[date_col].dt.is_month_start.astype(int)\n",
    "    df['is_month_end'] = df[date_col].dt.is_month_end.astype(int)\n",
    "    \n",
    "    print(f\"\\nâœ… Extracted date features from '{date_col}'\")\n",
    "    print(f\"   - Year, Month, Day, Day of Week, Quarter, Week\")\n",
    "    print(f\"   - Weekend flag, Month start/end flags\")\n",
    "    print(f\"\\nğŸ“… Date Range: {df[date_col].min()} to {df[date_col].max()}\")\n",
    "    print(f\"ğŸ“… Total Days: {(df[date_col].max() - df[date_col].min()).days}\")\n",
    "else:\n",
    "    print(\"âš ï¸  No order date column found\")\n",
    "\n",
    "# Process shipping date if exists\n",
    "if 'shipping_date_dateorders' in df.columns or 'shipping_date' in df.columns:\n",
    "    ship_date_col = 'shipping_date_dateorders' if 'shipping_date_dateorders' in df.columns else 'shipping_date'\n",
    "    df[ship_date_col] = pd.to_datetime(df[ship_date_col], errors='coerce')\n",
    "    print(f\"\\nâœ… Processed shipping date: {ship_date_col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.2 SUPPLY CHAIN TIER FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Creating supply chain tier features...\\n\")\n",
    "\n",
    "# Define supply chain hierarchy\n",
    "# Tier 1: Customer/Retailer (most downstream)\n",
    "# Tier 2: Department/Store\n",
    "# Tier 3: Market\n",
    "# Tier 4: Region (most upstream)\n",
    "\n",
    "tier_mapping = {\n",
    "    'customer_segment': 'tier_1_customer',\n",
    "    'department_name': 'tier_2_department',\n",
    "    'market': 'tier_3_market',\n",
    "    'order_region': 'tier_4_region'\n",
    "}\n",
    "\n",
    "# Create tier features\n",
    "for original_col, new_col in tier_mapping.items():\n",
    "    if original_col in df.columns:\n",
    "        df[new_col] = df[original_col]\n",
    "        print(f\"âœ… Created {new_col} from {original_col}\")\n",
    "\n",
    "# Create aggregated tier identifier\n",
    "tier_cols = [v for k, v in tier_mapping.items() if k in df.columns]\n",
    "if tier_cols:\n",
    "    df['supply_chain_path'] = df[tier_cols].astype(str).agg('_'.join, axis=1)\n",
    "    print(f\"\\nâœ… Created supply_chain_path combining all tiers\")\n",
    "    print(f\"   Unique paths: {df['supply_chain_path'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.3 VARIABILITY METRICS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Calculating variability metrics...\\n\")\n",
    "\n",
    "if 'order_item_quantity' in df.columns:\n",
    "    # Calculate rolling statistics for order quantity\n",
    "    # Group by customer and calculate rolling metrics\n",
    "    \n",
    "    if 'customer_id' in df.columns and 'order_date_dateorders' in df.columns:\n",
    "        # Sort by customer and date\n",
    "        df_sorted = df.sort_values(['customer_id', 'order_date_dateorders'])\n",
    "        \n",
    "        # Calculate rolling mean and std (window=3)\n",
    "        df_sorted['quantity_rolling_mean_3'] = df_sorted.groupby('customer_id')['order_item_quantity'].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).mean()\n",
    "        )\n",
    "        df_sorted['quantity_rolling_std_3'] = df_sorted.groupby('customer_id')['order_item_quantity'].transform(\n",
    "            lambda x: x.rolling(window=3, min_periods=1).std()\n",
    "        )\n",
    "        \n",
    "        # Coefficient of variation\n",
    "        df_sorted['quantity_cv'] = df_sorted['quantity_rolling_std_3'] / (df_sorted['quantity_rolling_mean_3'] + 1e-6)\n",
    "        \n",
    "        # Merge back to original df\n",
    "        df = df_sorted\n",
    "        \n",
    "        print(\"âœ… Created rolling variability metrics:\")\n",
    "        print(\"   - quantity_rolling_mean_3\")\n",
    "        print(\"   - quantity_rolling_std_3\")\n",
    "        print(\"   - quantity_cv (coefficient of variation)\")\n",
    "    \n",
    "    # Calculate order size categories\n",
    "    df['order_size_category'] = pd.cut(\n",
    "        df['order_item_quantity'],\n",
    "        bins=[0, 1, 3, 5, 10, float('inf')],\n",
    "        labels=['Very Small', 'Small', 'Medium', 'Large', 'Very Large']\n",
    "    )\n",
    "    print(\"\\nâœ… Created order_size_category\")\n",
    "    print(df['order_size_category'].value_counts())\n",
    "\n",
    "# Calculate discount impact\n",
    "if 'order_item_discount_rate' in df.columns:\n",
    "    df['has_discount'] = (df['order_item_discount_rate'] > 0).astype(int)\n",
    "    df['discount_category'] = pd.cut(\n",
    "        df['order_item_discount_rate'],\n",
    "        bins=[-0.01, 0, 0.1, 0.2, 0.5, 1.0],\n",
    "        labels=['No Discount', 'Low (0-10%)', 'Medium (10-20%)', 'High (20-50%)', 'Very High (>50%)']\n",
    "    )\n",
    "    print(\"\\nâœ… Created discount features\")\n",
    "    print(df['discount_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.4 CATEGORICAL ENCODING\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Encoding categorical variables...\\n\")\n",
    "\n",
    "# Create label encoders dictionary to save for later use\n",
    "label_encoders = {}\n",
    "\n",
    "# Categorical columns to encode\n",
    "categorical_cols_to_encode = [\n",
    "    'shipping_mode',\n",
    "    'customer_segment',\n",
    "    'market',\n",
    "    'order_region',\n",
    "    'category_name',\n",
    "    'department_name',\n",
    "    'order_status',\n",
    "    'delivery_status'\n",
    "]\n",
    "\n",
    "# Filter to only existing columns\n",
    "categorical_cols_to_encode = [col for col in categorical_cols_to_encode if col in df.columns]\n",
    "\n",
    "# Apply label encoding\n",
    "for col in categorical_cols_to_encode:\n",
    "    le = LabelEncoder()\n",
    "    df[f'{col}_encoded'] = le.fit_transform(df[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    print(f\"âœ… Encoded {col} -> {col}_encoded ({df[col].nunique()} categories)\")\n",
    "\n",
    "print(f\"\\nâœ… Total categorical features encoded: {len(categorical_cols_to_encode)}\")\n",
    "print(f\"\\nğŸ“Š Current DataFrame shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 3.5 FEATURE SUMMARY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"ğŸ“Š FEATURE ENGINEERING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "new_features = [col for col in df.columns if any([\n",
    "    col.startswith('order_'),\n",
    "    col.startswith('tier_'),\n",
    "    col.startswith('quantity_'),\n",
    "    col.endswith('_encoded'),\n",
    "    col.endswith('_category'),\n",
    "    col in ['is_weekend', 'has_discount', 'shipping_delay', 'supply_chain_path']\n",
    "])]\n",
    "\n",
    "print(f\"\\nâœ… Total new features created: {len(new_features)}\")\n",
    "print(\"\\nNew feature categories:\")\n",
    "print(f\"  - Date/Time features: {len([f for f in new_features if f.startswith('order_')])}\")\n",
    "print(f\"  - Supply chain tiers: {len([f for f in new_features if f.startswith('tier_')])}\")\n",
    "print(f\"  - Variability metrics: {len([f for f in new_features if f.startswith('quantity_')])}\")\n",
    "print(f\"  - Encoded features: {len([f for f in new_features if f.endswith('_encoded')])}\")\n",
    "print(f\"  - Category features: {len([f for f in new_features if f.endswith('_category')])}\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Final DataFrame shape: {df.shape}\")\n",
    "print(\"\\nâœ… Feature engineering complete!\")\n",
    "\n",
    "# Save processed dataframe\n",
    "df.to_csv('processed_supply_chain_data.csv', index=False)\n",
    "print(\"\\nğŸ’¾ Saved processed data to 'processed_supply_chain_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 4: BULLWHIP EFFECT ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.1 MULTI-TIER VARIANCE CALCULATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Calculating Bullwhip Effect across supply chain tiers...\\n\")\n",
    "\n",
    "def calculate_bullwhip_effect(df, tiers, quantity_col='order_item_quantity'):\n",
    "    \"\"\"\n",
    "    Calculate variance amplification across supply chain tiers\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Supply chain data\n",
    "    tiers : list\n",
    "        List of tier column names from downstream to upstream\n",
    "    quantity_col : str\n",
    "        Column name for order quantity\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary containing variance metrics per tier\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for tier in tiers:\n",
    "        if tier in df.columns:\n",
    "            # Aggregate orders by tier\n",
    "            tier_data = df.groupby(tier)[quantity_col].agg([\n",
    "                ('mean', 'mean'),\n",
    "                ('std', 'std'),\n",
    "                ('var', 'var'),\n",
    "                ('count', 'count'),\n",
    "                ('total', 'sum')\n",
    "            ]).reset_index()\n",
    "            \n",
    "            # Calculate coefficient of variation\n",
    "            tier_data['cv'] = tier_data['std'] / (tier_data['mean'] + 1e-6)\n",
    "            \n",
    "            results[tier] = {\n",
    "                'data': tier_data,\n",
    "                'overall_mean': df[quantity_col].mean(),\n",
    "                'overall_std': df[quantity_col].std(),\n",
    "                'overall_var': df[quantity_col].var(),\n",
    "                'overall_cv': df[quantity_col].std() / (df[quantity_col].mean() + 1e-6),\n",
    "                'num_entities': tier_data.shape[0]\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define supply chain tiers (from downstream to upstream)\n",
    "tiers = [\n",
    "    'tier_1_customer',      # Most downstream (retailer/customer level)\n",
    "    'tier_2_department',    # Department level\n",
    "    'tier_3_market',        # Market level\n",
    "    'tier_4_region'         # Most upstream (regional level)\n",
    "]\n",
    "\n",
    "# Filter to existing tiers\n",
    "existing_tiers = [t for t in tiers if t in df.columns]\n",
    "\n",
    "if existing_tiers and 'order_item_quantity' in df.columns:\n",
    "    # Calculate bullwhip effect\n",
    "    bullwhip_results = calculate_bullwhip_effect(df, existing_tiers)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸŒŠ BULLWHIP EFFECT ANALYSIS RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for tier in existing_tiers:\n",
    "        if tier in bullwhip_results:\n",
    "            result = bullwhip_results[tier]\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"ğŸ“ {tier.upper()}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            print(f\"Number of entities: {result['num_entities']}\")\n",
    "            print(f\"Overall Mean: {result['overall_mean']:.2f}\")\n",
    "            print(f\"Overall Std Dev: {result['overall_std']:.2f}\")\n",
    "            print(f\"Overall Variance: {result['overall_var']:.2f}\")\n",
    "            print(f\"Coefficient of Variation: {result['overall_cv']:.4f}\")\n",
    "    \n",
    "    # Calculate amplification ratios between tiers\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"ğŸ“Š VARIANCE AMPLIFICATION RATIOS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    amplification_ratios = []\n",
    "    for i in range(len(existing_tiers) - 1):\n",
    "        downstream_tier = existing_tiers[i]\n",
    "        upstream_tier = existing_tiers[i + 1]\n",
    "        \n",
    "        if downstream_tier in bullwhip_results and upstream_tier in bullwhip_results:\n",
    "            downstream_var = bullwhip_results[downstream_tier]['overall_var']\n",
    "            upstream_var = bullwhip_results[upstream_tier]['overall_var']\n",
    "            \n",
    "            ratio = upstream_var / (downstream_var + 1e-6)\n",
    "            amplification_ratios.append({\n",
    "                'from': downstream_tier,\n",
    "                'to': upstream_tier,\n",
    "                'ratio': ratio\n",
    "            })\n",
    "            \n",
    "            print(f\"\\n{downstream_tier} â†’ {upstream_tier}\")\n",
    "            print(f\"  Downstream Variance: {downstream_var:.2f}\")\n",
    "            print(f\"  Upstream Variance: {upstream_var:.2f}\")\n",
    "            print(f\"  ğŸŒŠ Amplification Ratio: {ratio:.2f}x\")\n",
    "            \n",
    "            if ratio > 1.5:\n",
    "                print(f\"  âš ï¸  HIGH BULLWHIP EFFECT DETECTED!\")\n",
    "            elif ratio > 1.0:\n",
    "                print(f\"  âš¡ Moderate bullwhip effect\")\n",
    "            else:\n",
    "                print(f\"  âœ… Low bullwhip effect\")\n",
    "    \n",
    "    # Calculate total amplification\n",
    "    if amplification_ratios:\n",
    "        total_amplification = np.prod([r['ratio'] for r in amplification_ratios])\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ¯ TOTAL AMPLIFICATION: {total_amplification:.2f}x\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"\\nThis means orders at the most upstream tier ({existing_tiers[-1]})\")\n",
    "        print(f\"have {total_amplification:.2f}x the variance of customer demand.\")\n",
    "else:\n",
    "    print(\"âš ï¸  Required columns not found for bullwhip analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.2 VISUALIZATION - VARIANCE BY TIER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "if existing_tiers and 'order_item_quantity' in df.columns:\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Variance by Tier\n",
    "    tier_names = []\n",
    "    variances = []\n",
    "    std_devs = []\n",
    "    cvs = []\n",
    "    \n",
    "    for tier in existing_tiers:\n",
    "        if tier in bullwhip_results:\n",
    "            tier_names.append(tier.replace('tier_', '').replace('_', ' ').title())\n",
    "            variances.append(bullwhip_results[tier]['overall_var'])\n",
    "            std_devs.append(bullwhip_results[tier]['overall_std'])\n",
    "            cvs.append(bullwhip_results[tier]['overall_cv'])\n",
    "    \n",
    "    # Variance plot\n",
    "    x_pos = np.arange(len(tier_names))\n",
    "    axes[0].bar(x_pos, variances, color=['green', 'yellow', 'orange', 'red'][:len(tier_names)],\n",
    "                edgecolor='black', linewidth=1.5)\n",
    "    axes[0].set_xlabel('Supply Chain Tier', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Variance', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Order Quantity Variance by Supply Chain Tier\\n(Bullwhip Effect Visualization)',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[0].set_xticks(x_pos)\n",
    "    axes[0].set_xticklabels(tier_names, rotation=45, ha='right')\n",
    "    axes[0].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, v in enumerate(variances):\n",
    "        axes[0].text(i, v, f'{v:.1f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # Coefficient of Variation plot\n",
    "    axes[1].plot(tier_names, cvs, marker='o', linewidth=2, markersize=10, color='red')\n",
    "    axes[1].fill_between(range(len(tier_names)), cvs, alpha=0.3, color='red')\n",
    "    axes[1].set_xlabel('Supply Chain Tier', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Coefficient of Variation (CV)', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Demand Variability Amplification\\n(Higher CV = More Bullwhip)',\n",
    "                      fontsize=14, fontweight='bold')\n",
    "    axes[1].set_xticklabels(tier_names, rotation=45, ha='right')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, cv in enumerate(cvs):\n",
    "        axes[1].text(i, cv, f'{cv:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('bullwhip_effect_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Bullwhip effect visualizations created!\")\n",
    "    print(\"ğŸ’¾ Saved as 'bullwhip_effect_analysis.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 4.3 ANOMALY DETECTION - ISOLATION FOREST\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Detecting demand anomalies using Isolation Forest...\\n\")\n",
    "\n",
    "if 'order_item_quantity' in df.columns:\n",
    "    # Prepare features for anomaly detection\n",
    "    anomaly_features = ['order_item_quantity']\n",
    "    \n",
    "    # Add more features if available\n",
    "    optional_features = ['sales', 'order_item_discount_rate', 'days_for_shipping_real']\n",
    "    anomaly_features.extend([f for f in optional_features if f in df.columns])\n",
    "    \n",
    "    # Create feature matrix\n",
    "    X_anomaly = df[anomaly_features].copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    X_anomaly = X_anomaly.fillna(X_anomaly.median())\n",
    "    \n",
    "    # Fit Isolation Forest\n",
    "    iso_forest = IsolationForest(\n",
    "        contamination=0.05,  # Expect 5% anomalies\n",
    "        random_state=42,\n",
    "        n_estimators=100\n",
    "    )\n",
    "    \n",
    "    # Predict anomalies (-1 = anomaly, 1 = normal)\n",
    "    df['is_anomaly'] = iso_forest.fit_predict(X_anomaly)\n",
    "    df['anomaly_score'] = iso_forest.score_samples(X_anomaly)\n",
    "    \n",
    "    # Analysis\n",
    "    n_anomalies = (df['is_anomaly'] == -1).sum()\n",
    "    pct_anomalies = (n_anomalies / len(df)) * 100\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ” ANOMALY DETECTION RESULTS\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Total orders analyzed: {len(df):,}\")\n",
    "    print(f\"Anomalies detected: {n_anomalies:,} ({pct_anomalies:.2f}%)\")\n",
    "    print(f\"Normal orders: {(df['is_anomaly'] == 1).sum():,}\")\n",
    "    \n",
    "    # Analyze anomalies\n",
    "    anomalies_df = df[df['is_anomaly'] == -1]\n",
    "    \n",
    "    print(\"\\nğŸ“Š Anomaly Characteristics:\")\n",
    "    print(f\"  Avg Quantity (Anomalies): {anomalies_df['order_item_quantity'].mean():.2f}\")\n",
    "    print(f\"  Avg Quantity (Normal): {df[df['is_anomaly'] == 1]['order_item_quantity'].mean():.2f}\")\n",
    "    \n",
    "    if 'sales' in df.columns:\n",
    "        print(f\"  Avg Sales (Anomalies): ${anomalies_df['sales'].mean():.2f}\")\n",
    "        print(f\"  Avg Sales (Normal): ${df[df['is_anomaly'] == 1]['sales'].mean():.2f}\")\n",
    "    \n",
    "    # Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Plot 1: Anomaly Score Distribution\n",
    "    axes[0].hist(df['anomaly_score'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(x=df[df['is_anomaly'] == -1]['anomaly_score'].max(), \n",
    "                    color='red', linestyle='--', linewidth=2, label='Anomaly Threshold')\n",
    "    axes[0].set_xlabel('Anomaly Score', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Anomaly Score Distribution', fontsize=14, fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Order Quantity - Normal vs Anomaly\n",
    "    normal_qty = df[df['is_anomaly'] == 1]['order_item_quantity']\n",
    "    anomaly_qty = df[df['is_anomaly'] == -1]['order_item_quantity']\n",
    "    \n",
    "    axes[1].hist(normal_qty, bins=50, alpha=0.7, label='Normal', color='green', edgecolor='black')\n",
    "    axes[1].hist(anomaly_qty, bins=50, alpha=0.7, label='Anomaly', color='red', edgecolor='black')\n",
    "    axes[1].set_xlabel('Order Quantity', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Order Quantity Distribution: Normal vs Anomalies', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('anomaly_detection_results.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Anomaly detection complete!\")\n",
    "    print(\"ğŸ’¾ Saved visualization as 'anomaly_detection_results.png'\")\n",
    "    \n",
    "    # Save anomalies for further analysis\n",
    "    anomalies_df.to_csv('detected_anomalies.csv', index=False)\n",
    "    print(\"ğŸ’¾ Saved anomalies to 'detected_anomalies.csv'\")\n",
    "else:\n",
    "    print(\"âš ï¸  order_item_quantity column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# SECTION 5: DEMAND FORECASTING (LSTM)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# 5.1 DATA PREPARATION FOR TIME SERIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"Preparing data for LSTM demand forecasting...\\n\")\n",
    "\n",
    "# Check required columns\n",
    "date_col = None\n",
    "for col in df.columns:\n",
    "    if 'order_date' in col.lower() and 'dateorders' in col.lower():\n",
    "        date_col = col\n",
    "        break\n",
    "\n",
    "if date_col and 'order_item_quantity' in df.columns:\n",
    "    # Create daily aggregated data\n",
    "    df_ts = df.copy()\n",
    "    df_ts[date_col] = pd.to_datetime(df_ts[date_col])\n",
    "    df_ts = df_ts.sort_values(date_col)\n",
    "    \n",
    "    # Aggregate by date\n",
    "    daily_demand = df_ts.groupby(date_col).agg({\n",
    "        'order_item_quantity': 'sum',\n",
    "        'sales': 'sum' if 'sales' in df_ts.columns else 'count',\n",
    "        'order_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    daily_demand.columns = ['date', 'total_quantity', 'total_sales', 'num_orders']\n",
    "    \n",
    "    # Fill missing dates\n",
    "    date_range = pd.date_range(start=daily_demand['date'].min(), \n",
    "                                end=daily_demand['date'].max(), \n",
    "                                freq='D')\n",
    "    daily_demand = daily_demand.set_index('date').reindex(date_range, fill_value=0).reset_index()\n",
    "    daily_demand.columns = ['date', 'total_quantity', 'total_sales', 'num_orders']\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“Š TIME SERIES DATA SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Date range: {daily_demand['date'].min()} to {daily_demand['date'].max()}\")\n",
    "    print(f\"Total days: {len(daily_demand)}\")\n",
    "    print(f\"Total demand: {daily_demand['total_quantity'].sum():,.0f}\")\n",
    "    print(f\"Average daily demand: {daily_demand['total_quantity'].mean():.2f}\")\n",
    "    print(f\"Max daily demand: {daily_demand['total_quantity'].max():.0f}\")\n",
    "    print(f\"Min daily demand: {daily_demand['total_quantity'].min():.0f}\")\n",
    "    \n",
    "    # Visualize time series\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(16, 10))\n",
    "    \n",
    "    # Daily demand plot\n",
    "    axes[0].plot(daily_demand['date'], daily_demand['total_quantity'], linewidth=1.5, color='blue')\n",
    "    axes[0].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_ylabel('Total Quantity', fontsize=12, fontweight='bold')\n",
    "    axes[0].set_title('Daily Demand Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Rolling average (7-day)\n",
    "    daily_demand['rolling_mean_7'] = daily_demand['total_quantity'].rolling(window=7).mean()\n",
    "    daily_demand['rolling_mean_30'] = daily_demand['total_quantity'].rolling(window=30).mean()\n",
    "    \n",
    "    axes[1].plot(daily_demand['date'], daily_demand['total_quantity'], \n",
    "                 linewidth=1, color='lightblue', alpha=0.5, label='Daily')\n",
    "    axes[1].plot(daily_demand['date'], daily_demand['rolling_mean_7'], \n",
    "                 linewidth=2, color='orange', label='7-day MA')\n",
    "    axes[1].plot(daily_demand['date'], daily_demand['rolling_mean_30'], \n",
    "                 linewidth=2, color='red', label='30-day MA')\n",
    "    axes[1].set_xlabel('Date', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_ylabel('Total Quantity', fontsize=12, fontweight='bold')\n",
    "    axes[1].set_title('Demand with Moving Averages', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('time_series_demand.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nâœ… Time series visualization created!\")\n",
    "else:\n",
    "    print(\"âš ï¸  Required columns for time series analysis not found\")\n",
    "    daily_demand = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
